{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msal\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "wd = os.getcwd()\n",
    "\n",
    "def get_app_token(username: str, password: str, client_id: str, tenant_id: str) -> str:\n",
    "    '''\n",
    "    Retrieve token for the app registered in Azure & PowerBI Service.\n",
    "    \n",
    "    Parameters:\n",
    "        username (str): for Azure & PowerBI account\n",
    "        password (str): for Azure & PowerBI account\n",
    "        client_id (str): Azure App Registration (client) ID\n",
    "        tenant_id (str): Azure App Registration directory (tenant) ID\n",
    "    \n",
    "    Returns:\n",
    "        access_token (str): token for accessing PBI Service API\n",
    "        \n",
    "    '''\n",
    "    authority_url = 'https://login.microsoftonline.com/' + tenant_id\n",
    "    scope = ['https://analysis.windows.net/powerbi/api/.default']\n",
    "    \n",
    "    \n",
    "    app = msal.PublicClientApplication(client_id, authority=authority_url)\n",
    "    result = app.acquire_token_by_username_password(username=username,password=password,scopes=scope)\n",
    "    access_token = result['access_token']\n",
    "        \n",
    "    return access_token\n",
    "    \n",
    "def download_content_df(access_token: str, url_extension = 'groups') -> pd.DataFrame:\n",
    "    '''\n",
    "    Downloading specific entity data from PBI service.\n",
    "    \n",
    "    Parameters:\n",
    "        access_token (str): token allowing connection to PBI Service API\n",
    "        url_extension (str): specification which entity download\n",
    "    \n",
    "    Returns:\n",
    "        content_df (pd.DataFrame): downloaded data\n",
    "    '''\n",
    "    \n",
    "    url_groups = 'https://api.powerbi.com/v1.0/myorg/' + url_extension\n",
    "    header = {'Content-Type':'application/json','Authorization': f'Bearer {access_token}'}\n",
    "    \n",
    "    api_out = requests.get(url=url_groups, headers=header)\n",
    "    content_df = pd.DataFrame(api_out.json()['value'])\n",
    "    content_df = content_df.rename(columns = {'objectId': 'id'})\n",
    "    \n",
    "    return content_df\n",
    "\n",
    "def download_specific_content_df(url_base:str, resources_ids: pd.Series, content_type: str, token: str, data_category: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Having list of resources of the entity (for example dataflows within workspace), iterate over the resources\n",
    "    to retrieve each resource specific information (in this case - datasources) and concatenate them into one dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        url_base (str): first part of url pointing on the entity\n",
    "        resources_ids (pd.Series): collection of resources to iterate on (part of url)\n",
    "        content_type (str): category of entities to iterate on (part of url), it may be tiles or datasources\n",
    "        token (str): access token\n",
    "        data_category (str): datasource category (dataflows, datasets, dashboards)\n",
    "    \n",
    "    Returns:\n",
    "        merged_df (pd.DataFrame): downloaded and merged data\n",
    "    '''\n",
    "    \n",
    "    merged_df = pd.DataFrame()\n",
    "    for resource_id in resources_ids:\n",
    "        url_base_ext = url_base + f'/{resource_id}/{content_type}'\n",
    "        content_spec = download_content_df(token, url_base_ext)\n",
    "        content_spec[data_category+'Id'] = resource_id\n",
    "        merged_df = merged_df.append(content_spec)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def retrieve_data_set_or_flow_sources(data_dict: dict, data_type: str, data_params: tuple) -> pd.DataFrame:\n",
    "    '''\n",
    "    Extract datasources (flows or sets) dataframe from dictionary and transform into draw.io-digestible format.\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "        data_dict (dict): dictionary with key-dataframe pairs\n",
    "        datatype (str): type of entity (dataflows or datasets)\n",
    "        data_params (tuple): collection of workspace id and name.\n",
    "    \n",
    "    Returns:\n",
    "        df (pd.DataFrame): transformed datasources dataframe.\n",
    "    '''\n",
    "    \n",
    "    df = data_dict[data_type + '_datasources']\n",
    "    \n",
    "    if not df.empty:\n",
    "        df = df.loc[:,['datasourceType', data_type + 'Id','connectionDetails']] #\n",
    "        df['connectionDetails'] = df['connectionDetails'].astype(str)\n",
    "        df = df.rename(columns = {'connectionDetails': 'id',\n",
    "                                  'datasourceType': 'name', \n",
    "                                  data_type + 'Id': 'parent'}) \n",
    "        df = df.groupby(['id','name'])['parent'].apply(join_strings).reset_index()\n",
    "        df['type'] = data_type + '_datasources'\n",
    "        return df\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def select_groups(df, groups: list) -> pd.DataFrame:\n",
    "    '''\n",
    "    Select subset of workspaces defined by name in the list, or get all workspaces when list is empty.\n",
    "    '''\n",
    "    if not groups:\n",
    "        df = df.loc[:,['name', 'id']]\n",
    "    else:\n",
    "        df = df[df['name'].isin(groups)][['name', 'id']]\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def join_strings(collection):\n",
    "    '''\n",
    "    Create string sequence of unique objects from collection.\n",
    "    '''\n",
    "    return ','.join(set(collection))\n",
    "\n",
    "\n",
    "drawio_spec = \\\n",
    "'''# label: %name%<br><i>%type%</i>\n",
    "#\n",
    "# style: label;image=%image%;whiteSpace=wrap;html=1;rounded=1;fillColor=%fill%;horizontal=1;\n",
    "#\n",
    "# parentstyle: swimlane;whiteSpace=wrap;html=1;childLayout=stackLayout;horizontal=1;horizontalStack=1;resizeParent=1;resizeLast=0;collapsible=1;\n",
    "#\n",
    "# namespace: csvimport-\n",
    "#\n",
    "# connect: {\"from\": \"parent\", \"to\": \"id\", \"invert\": true, \"style\": \"curved=1;endArrow=none;endFill=1;fontSize=11;\"}\n",
    "# connect: {\"from\": \"relatives\", \"to\": \"id\", \"style\": \"curved=1;fontSize=11;dashed=1;endArrow=none;\"}\n",
    "#\n",
    "## layout: horizontalflow\n",
    "# nodespacing: 150\n",
    "# levelspacing: 200\n",
    "# edgespacing: 150\n",
    "# ignore: image,fill\n",
    "'''\n",
    "\n",
    "html_spec = \\\n",
    "'''\n",
    "type\tfill\timage\n",
    "workspaces\t#ff6666\thttps://img.icons8.com/carbon-copy/100/000000/power-plant.png\n",
    "dataflows\t#00ccff\thttps://img.icons8.com/ios/50/000000/database-import.png\n",
    "datasets\t#33cc33\thttps://img.icons8.com/ios/50/000000/database.png\n",
    "reports\t#ffff99\thttps://img.icons8.com/ios/50/000000/business-report.png\n",
    "dashboards\t#ff9900\thttps://img.icons8.com/dotty/80/000000/dashboard.png\n",
    "users\t#ffffff\thttps://img.icons8.com/windows/32/000000/user-male-circle.png\n",
    "dataflows_datasources\t#b3cccc\t\n",
    "datasets_datasources\t#c2c2d6\t\n",
    "'''\n",
    "\n",
    "html_spec = pd.read_csv(StringIO(html_spec), sep ='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = get_app_token(username='FILL THIS VALUE', password='FILL THIS VALUE', \n",
    "                      client_id='FILL THIS VALUE', \n",
    "                      tenant_id='FILL THIS VALUE')\n",
    "group_df = download_content_df(token, 'groups')\n",
    "selected_groups = select_groups(group_df, ['FILL THIS LIST']) #\n",
    "output_all = pd.DataFrame()\n",
    "\n",
    "for row in selected_groups.itertuples():\n",
    "    print(row.name)\n",
    "\n",
    "    # placeholders\n",
    "    output = pd.DataFrame()\n",
    "    data_dict = {}\n",
    "    missing_cat = []\n",
    "\n",
    "    # loop for data download\n",
    "    for cat in ['users', 'dataflows', 'datasets', 'reports', 'dashboards']:\n",
    "        url_base = f'groups/{row.id}/{cat}'\n",
    "        content = download_content_df(token, url_base)\n",
    "\n",
    "        # if category is missing, note it and continue to next\n",
    "        if content.empty:\n",
    "            missing_cat.append(cat)\n",
    "            continue\n",
    "        else:\n",
    "            data_dict[cat] = content\n",
    "\n",
    "        if cat in ['dataflows', 'datasets']:\n",
    "            content_ext = download_specific_content_df(url_base, content['id'], 'datasources', token, cat) \n",
    "            data_dict[cat+'_datasources'] = content_ext\n",
    "\n",
    "        if cat in ['datasets']:\n",
    "            # get links from dataset to dataflows\n",
    "            url_base_ext = url_base + f'/upstreamdataflows'\n",
    "            content_ext = download_content_df(token, url_base_ext)\n",
    "            data_dict[cat+'_upstreamdataflows'] = content_ext\n",
    "\n",
    "        if cat in ['dashboards']:\n",
    "            content_ext = download_specific_content_df(url_base, content['id'], 'tiles', token, cat) \n",
    "            data_dict[cat+'_datasources'] = content_ext\n",
    "\n",
    "    # if workspace is empty (has only usage-monitoring datasets and reports), continue to the next workspace\n",
    "    if 'identifier' not in data_dict['users'].columns:\n",
    "        continue\n",
    "\n",
    "    ''' USERS '''\n",
    "\n",
    "    if 'users' not in missing_cat:\n",
    "        users = data_dict['users']\n",
    "        users['id'] = users['groupUserAccessRight'] + ':' + users['identifier']\n",
    "        users = users.loc[:,['id', 'displayName']]\n",
    "        users = users[users['id'].str.contains('@')]\n",
    "        users['type'] = 'users'\n",
    "        users = users.rename(columns = {'displayName': 'name'})\n",
    "\n",
    "        output = pd.concat([output, users])\n",
    "\n",
    "    ''' DATAFLOWS '''\n",
    "\n",
    "    if 'dataflows' not in missing_cat:\n",
    "        dataflows = data_dict['dataflows']\n",
    "        dataflows = dataflows.loc[:,['id', 'name']]\n",
    "        dataflows['type'] = 'dataflows'\n",
    "        dataflows['parent'] = row.id\n",
    "\n",
    "        ''' DATAFLOWS DATASOURCES '''\n",
    "\n",
    "        dataflows_datasources = retrieve_data_set_or_flow_sources(data_dict, 'dataflows', row)\n",
    "\n",
    "        output = pd.concat([output, dataflows, dataflows_datasources])\n",
    "\n",
    "    ''' DATASETS '''\n",
    "\n",
    "    if 'datasets' not in missing_cat:\n",
    "        datasets = data_dict['datasets']\n",
    "        datasets = datasets.loc[:,['id', 'name', 'configuredBy']]\n",
    "        datasets['type'] = 'datasets'\n",
    "        datasets = datasets.rename(columns = {'configuredBy': 'relatives'})\n",
    "\n",
    "        datasets_upstream = data_dict['datasets_upstreamdataflows']\n",
    "#         shared_workspaces = list(datasets_upstream['workspaceObjectId'].unique())\n",
    "#         shared_workspaces.remove(row.id)\n",
    "\n",
    "        # upstream is empty, when there are no original datasets (only shared datasets), but other resources exist\n",
    "        if not datasets_upstream.empty:\n",
    "            datasets_upstream = datasets_upstream.groupby('datasetObjectId').agg({'dataflowObjectId': join_strings,\n",
    "                                                        'workspaceObjectId': join_strings}).reset_index()\n",
    "            datasets_upstream = datasets_upstream.rename(columns = {'datasetObjectId': 'id', 'dataflowObjectId': 'relatives',\n",
    "                                                                    'workspaceObjectId': 'parent'})\n",
    "            datasets = pd.merge(datasets, datasets_upstream, on = 'id', how = 'left')\n",
    "            datasets['parent'] = datasets['parent'].fillna(row.id)\n",
    "            datasets['relatives'] = (datasets['relatives_x'] + ',' + datasets['relatives_y'].fillna(',')).replace(r',,', '', regex=True)\n",
    "            datasets = datasets.drop(columns = ['relatives_x', 'relatives_y'])\n",
    "        else:\n",
    "            datasets['parent'] = row.id\n",
    "\n",
    "        ''' DATASETS DATASOURCES '''\n",
    "\n",
    "        datasets_datasources = retrieve_data_set_or_flow_sources(data_dict, 'datasets', row)\n",
    "\n",
    "        output = pd.concat([output, datasets, datasets_datasources])     \n",
    "\n",
    "    ''' REPORTS '''\n",
    "\n",
    "    if 'reports' not in missing_cat:\n",
    "        reports = data_dict['reports']\n",
    "        reports = reports.loc[:,['id', 'name', 'datasetId']]\n",
    "        reports['type'] = 'reports'\n",
    "        reports = reports.rename(columns = {'datasetId': 'parent'})\n",
    "        reports['parent'] = row.id + ',' + reports['parent']\n",
    "\n",
    "        output = pd.concat([output, reports])\n",
    "\n",
    "    ''' DASHBOARDS '''\n",
    "\n",
    "    if 'dashboards' not in missing_cat:\n",
    "        dashboards = data_dict['dashboards']\n",
    "        dashboards = dashboards.loc[:,['id', 'displayName']]\n",
    "        dashboards['type'] = 'dashboards'\n",
    "        dashboards = dashboards.rename(columns = {'displayName': 'name'})\n",
    "\n",
    "        dashboards_datasources = data_dict['dashboards_datasources'].fillna(',')\n",
    "        dashboards_datasources = dashboards_datasources.groupby('dashboardsId').agg({'reportId': join_strings,\n",
    "                                                            'datasetId': join_strings}).reset_index().replace(r',,', '', regex=True)\n",
    "        dashboards_datasources['parent'] = dashboards_datasources['reportId'] + ',' + dashboards_datasources['datasetId']\n",
    "        dashboards_datasources = dashboards_datasources.drop(columns = ['reportId', 'datasetId'])\n",
    "        dashboards_datasources = dashboards_datasources.rename(columns = {'dashboardsId': 'id'})\n",
    "\n",
    "        dashboards = pd.merge(dashboards, dashboards_datasources, on ='id', how = 'left')\n",
    "        dashboards['parent'] = dashboards['parent'].fillna(row.id) \n",
    "\n",
    "        output = pd.concat([output, dashboards])\n",
    "\n",
    "    ''' WORKSPACE & OUTPUT '''\n",
    "\n",
    "    # add workspace row to the dataframe\n",
    "    output = output.append({'id': row.id, 'name':row.name, 'type': 'workspaces', 'relatives': join_strings(users['id'].unique())},\n",
    "                        ignore_index=True)\n",
    "    # in case drawio has problems with reading special characters, take ids between quotation marks\n",
    "    output['id'] = '\"' + output['id'] + '\"'\n",
    "    output = pd.merge(output, html_spec, on = 'type', how = 'left')\n",
    "\n",
    "    output_all = pd.concat([output_all, output])\n",
    "\n",
    "# avoid duplicates and sort df for easier version controll\n",
    "output_all = output_all.drop_duplicates().sort_values(by = ['type', 'id'])\n",
    "\n",
    "# saave df as csv and create txt input file for draw.io\n",
    "output_all.to_csv(wd + '/drawio_relationships.csv', index = False, sep = ',', encoding = 'CP1250')\n",
    "with open(wd + '/drawio_input.txt', 'w') as file:\n",
    "    file.write(drawio_spec)\n",
    "    file.write(output_all.to_csv(index = False, sep = ',', encoding = 'CP1250').replace('\\n','').replace('\"\"\"', '\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}